#+OPTIONS: toc:nil email:nil H:4
#+TITLE: Hands-on ELK Workshop
#+AUTHOR: Marco Bertani-Ã˜kland and Sigmund Hansen
#+EMAIL: 
#+REVEAL_THEME: night

* Setup

- Extract at the root of the repository:
  - Elasticsearch
  - Logstash
  - Kibana

** Elasticsearch

- Auto-discovery of nodes in cluster
  - You will find the settings in *elasticsearch-2.1.1/config/elasticsearch.yml*
  - Change the value of *cluster.name* to something unique
  - Now you can run elasticsearch alongside other workshop participants

* Exercises

- Basic Logstash Setup
- LAPD Crime Reports
- HTTP Access Logs

All exercises are available in plain text, alongside the referenced
configuration files.

** Basic Logstash Setup

- Start a shell/cmd prompt
- Change to the *logstash/pipelines/setup* directory

*** Syntax checker

- Verify the syntactic correctness of the *verify.conf* configuration

Linux:
#+BEGIN_SRC bash
../../../logstash-2.1.1/bin/logstash agent -f verify.conf --configtest
#+END_SRC

Windows:
#+BEGIN_SRC bash
..\..\..\logstash-2.1.1\bin\logstash agent -f verify.conf --configtest
#+END_SRC

*** Test the configuration

- Run Logstash with the configuration with the command below
- Type one or more lines in the console
- Exit Logstash with *Ctrl+C*

Linux:
#+BEGIN_SRC bash
../../../logstash-2.1.1/bin/logstash agent -f verify.conf --configtest
#+END_SRC

Windows:
#+BEGIN_SRC bash
..\..\..\logstash-2.1.1\bin\logstash agent -f verify.conf --configtest
#+END_SRC

* LAPD Crime Reports

- Familiarize yourself with the data *logstash/pipelines/lapd/data/lapd_small.csv*
- We will focus on the following headers:

|-------------+------------------------|
| DATE OCC    | Date of occurrence     |
| TIME OCC    | Time of occurrence     |
| Crm Cd      | Crime Code             |
| Crm Cd Desc | Crime Code Description |
| Status      |                        |
| Statue Desc |                        |
| LOCATION    | Street address         |
| location 1  | GPS coordinates        |
|-------------+------------------------|

** File Input Plugin

** CSV Filter Plugin

** Clean and format your data

** Send to Elasticsearch

* HTTP Access Logs

Access logs generated by a script based on: \\
https://gist.github.com/fetep/2037301

Logs, exercises and configuration files can be found in *logstash/pipelines/httpd*

** Grok

- Regular expression text parser
- Pre-defined patterns
  - See: https://github.com/logstash-plugins/logstash-patterns-core/
- Named matches become fields

*** Getting started

- Have a look at *data/access.mini.log*
- Adapt the paths in *1.conf*
- Run logstash and take note of the *test* field:

Windows:
#+BEGIN_SRC bash
..\..\..\logstash-2.1.1\bin\logstash agent -f 1.conf
#+END_SRC

Linux:
#+BEGIN_SRC bash
../../../logstash-2.1.1/bin/logstash agent -f 1.conf
#+END_SRC

**** Match Option

+ Take note of the pattern used: *"%{DATA:test} "*
+ *DATA* is a pre-defined pattern equivalent to ".*?"
+ *:test* tells grok to bind the match to the field *test*
+ "%{DATA:test} " is equivalent to "(?<test>.*?) "

*** Grok constructor

- Regular expressions can be a hassle
- Lots of pre-defined patterns (around 120)
- http://grokconstructor.appspot.com/ \\
  to the rescue

**** Incremental Construction

- Select incremental construction
- Copy a few lines from access.mini.log into the text area and press Go
- Notice that the first pattern in the list matches everything: \\
  *COMBINEDAPACHELOG*
  - In the final results, we will use this pattern. \\
    For now, spend a few minutes getting familiar with the constructor.

**** Incremental Construction cont.

- The Apache log format documentation: \\
  https://httpd.apache.org/docs/1.3/logs.html#common
- Try to build a pattern that will capture the following fields:
  - Client IP/host name
  - Date and time
  - HTTP method
  - Path part of requested URL
  - HTTP status code
- Feel free to handle more parts
- Remember to add field names to the pattern
- Test your patterns

** Geo IP

- Adds GPS coordinates based on IP addresses.
- A database mapping IP addresses to cities is included in logstash.
- Updated databases can be downloaded from \\
  http://dev.maxmind.com/geoip/legacy/geolite/

*** Basic Geo IP Configuration

- Use *2.conf*, or add a geoip filter after your grok filter
- First set the source field to the client IP/host name field: \\
  *clientip* in the pre-defined pattern
- Try running logstash with the configuration

*** Fields

- The geoip has added a lot of fields
- The most important one is *[geoip][location]* (coordinates)
- All these fields take up additional storage space
- Add a field option to the geoip filter and input an array of fields you want to keep
- Re-run logstash with the updated configuration

** Timestamp

- Use *3.conf*, for this and the next exercise
- Format specification can be found at: \\
  http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html
- Add a date filter similar to the one used in the LAPD exercise
- You don't need to specify the time zone, \\
  because the Apache date format contains it

** Output to Elasticsearch

- Add output to Elasticsearch
- Set the name of the index

*** Import Full Access Log

- Unzip the *data/access.zip* archive
- Run logstash with the final configuration

* Wrap-up

** Unit/Integration Tests

- Testing Logstash configurations can be difficult
- It is possible to write unit tests in Ruby:
- http://stackoverflow.com/questions/18823917/how-to-implement-the-unit-or-integration-tests-for-logstash-configuration
** Time-based Indices

- You can add date fields to the index name
  - Slight increase in storage requirements
  - Allows deleting data
  - Increased performance
- You may want indices to be:
  - Daily: "-%{+YYYY.MM.dd}"
  - Weekly "-%{+xxxx.ww}"
  - Monthly "-%{+YYYY.MM}"
- Defaults to daily: "logstash-%{+YYYY.MM.dd}"
