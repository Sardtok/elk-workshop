#+OPTIONS: toc:nil email:nil
#+TITLE: Hands-on ELK Workshop
#+AUTHOR: Marco Bertani-Ã˜kland and Sigmund Hansen
#+EMAIL: 
#+REVEAL_THEME: night

* Setup

- Extract at the root of the repository:
  - Elasticsearch
  - Logstash
  - Kibana

** Elasticsearch

- Auto-discovery of nodes in cluster
  - You will find the settings in *elasticsearch-2.1.1/config/elasticsearch.yml*
  - Change the value of *cluster.name* to something unique
  - Now you can run elasticsearch alongside other workshop participants

* Exercises

- Basic Logstash Setup
- LAPD Crime Reports
- HTTP Access Logs

All exercises are available in plain text, alongside the referenced
configuration files.

** Basic Logstash Setup

- Start a shell/cmd prompt
- Change to the *logstash/pipelines/setup* directory

*** Syntax checker

- Verify the syntactic correctness of the *verify.conf* configuration

Linux:
#+BEGIN_SRC bash
../../../logstash-2.1.1/bin/logstash agent -f verify.conf --configtest
#+END_SRC

Windows:
#+BEGIN_SRC bash
..\..\..\logstash-2.1.1\bin\logstash agent -f verify.conf --configtest
#+END_SRC

*** Test the configuration

- Run Logstash with the configuration with the command below
- Type one or more lines in the console
- Exit Logstash with *Ctrl+C*

Linux:
#+BEGIN_SRC bash
../../../logstash-2.1.1/bin/logstash agent -f verify.conf --configtest
#+END_SRC

Windows:
#+BEGIN_SRC bash
..\..\..\logstash-2.1.1\bin\logstash agent -f verify.conf --configtest
#+END_SRC

* LAPD Crime Reports

- Familiarize yourself with the data *logstash/pipelines/lapd/data/lapd_small.csv*
- We will focus on the following headers:

|-------------+------------------------|
| DATE OCC    | Date of occurrence     |
| TIME OCC    | Time of occurrence     |
| Crm Cd      | Crime Code             |
| Crm Cd Desc | Crime Code Description |
| Status      |                        |
| Statue Desc |                        |
| LOCATION    | Street address         |
| location 1  | GPS coordinates        |
|-------------+------------------------|

** File Input Plugin

** CSV Filter Plugin

** Clean and format your data

** Send to Elasticsearch

* HTTP Access Logs

Access logs generated by a script based on: \\
https://gist.github.com/fetep/2037301

Logs, exercises and configuration files can be found in *logstash/pipelines/httpd*

** Grok

- Regular expression text parser
- Pre-defined patterns
- Named matches become fields

*** Getting started

- Have a look at *data/access.mini.log*
- Adapt the paths in *1.conf*
- Run logstash and take note of the *test* field:

Windows:
#+BEGIN_SRC bash
..\..\..\logstash-2.1.1\bin\logstash agent -f 1.conf
#+END_SRC

Linux:
#+BEGIN_SRC bash
../../../logstash-2.1.1/bin/logstash agent -f 1.conf
#+END_SRC

**** Match Option

+ Take note of the pattern used: *"%{DATA:test} "*
+ *DATA* is a pre-defined pattern equivalent to ".*?"
+ *:test* tells grok to bind the match to the field *test*
+ "%{DATA:test} " is equivalent to "(?<test>.*?) " 

*** Grok constructor

- Regular expressions can be a hassle
- Lots of pre-defined patterns (120 according to docs)
- http://grokconstructor.appspot.com/ \\
  to the rescue

****

** Geo IP

- Adds GPS coordinates based on IP addresses.
- A database mapping IP addresses to cities is included in logstash.
- Updated databases can be downloaded from \\
  http://dev.maxmind.com/geoip/legacy/geolite/

*** 

- 

** Timestamp

- Use the date and time specified in the logs
- Access logs do not use ISO-8601 formats
- Format specification can be found at: \\
  http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html

** Output to Elasticsearch

This is pretty similar to the last step of the LAPD exercise. But we
will not be changing the mappings this time around.

*** Index Name



* Wrap-up

** Unit/Integration Tests

- Testing Logstash configurations can be difficult
- It is possible to write unit tests in Ruby:
- http://stackoverflow.com/questions/18823917/how-to-implement-the-unit-or-integration-tests-for-logstash-configuration
